% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cutpointr.R
\name{cutpointr}
\alias{cutpointr}
\title{Determine and evaluate optimal cutpoints}
\usage{
cutpointr(data = NULL, x, class, subgroup = NULL,
  method = maximize_metric, metric = sum_sens_spec, pos_class = NULL,
  neg_class = NULL, direction = NULL, boot_runs = 0,
  use_midpoints = FALSE, na.rm = FALSE, allowParallel = FALSE,
  silent = FALSE, ...)
}
\arguments{
\item{data}{A data frame or tibble in which the columns that may be given in x,
class and possibly subgroup can be found.}

\item{x}{The variable name without quotation marks to be used for
classification, e.g. predictions or test values, or the raw data if data = NULL.}

\item{class}{The variable name without quotation marks indicating class membership
or the raw data if data = NULL.}

\item{subgroup}{The variable name without quotation marks
of an additional covariate that identifies subgroups or the raw data if
data = NULL. Separate optimal cutpoints will be determined by group.
Numeric, character and factor are allowed.}

\item{method}{(function) A function for determining cutpoints. Can
be user supplied or use some of the built in methods. See details.}

\item{metric}{(function) The function for computing a metric when using
maximize_metric or minimize_metric as method and and for the
out-of-bag values during bootstrapping. A way of internally validating the performance.
User defined functions can be supplied, see details.}

\item{pos_class}{(optional) The value of class that indicates the positive class.}

\item{neg_class}{(optional) The value of class that indicates the negative class.}

\item{direction}{(character, optional) Use ">=" or "<=" to indicate whether x
is supposed to be larger or smaller for the positive class.}

\item{boot_runs}{(numerical) If positive, this number of bootstrap samples
will be used to assess the variability and the out-of-sample performance.}

\item{use_midpoints}{(logical) If TRUE (default FALSE) the returned optimal
cutpoint will be the mean of the optimal cutpoint and the next highest
observation (for direction = ">") or the next lowest observation
(for direction = "<").}

\item{na.rm}{(logical) Set to TRUE (default FALSE) to keep only complete
cases of x, class and subgroup (if specified). Missing values with
na.rm = FALSE will raise an error.}

\item{allowParallel}{(logical) If TRUE, the bootstrapping will be parallelized
using foreach. A local cluster, for example, should have been started manually
beforehand.}

\item{silent}{(logical) If TRUE suppresses all messages.}

\item{...}{Further optional arguments that will be passed to method.
minimize_metric and maximize_metric pass ... to metric.}
}
\value{
A cutpointr object which is also a data.frame and tbl_df.
}
\description{
Using predictions (or e.g. biological marker values) and binary class labels, this function
will determine "optimal" cutpoints using various selectable methods. The
methods for cutpoint determination can be evaluated using bootstrapping. An
estimate of the cutpoint variability and the out-of-sample performance will
be returned.
}
\details{
If direction and/or pos_class and neg_class are not given, the function will
assume that higher values indicate the positive class and assign the class
with a higher mean as the positive class.

Different methods can be used for determining the "optimal" cutpoint via
the method argument. The package includes the following cutpoint functions:
\itemize{
 \item maximize_metric: Maximize the metric function
 \item minimize_metric: Minimize the metric function
 \item maximize_loess_metric: Maximize the metric function after LOESS
 smoothing
 \item minimize_loess_metric: Minimize the metric function after LOESS
 smoothing
 \item maximize_boot_metric: Maximize the metric function as a mean of
 the optimal cutpoints in bootstrapped samples
 \item minimize_boot_metric: Minimize the metric function as a mean of
 the optimal cutpoints in bootstrapped samples
 \item oc_manual: Specify the cutoff value manually
 \item oc_youden_kernel: Maximize the Youden-Index after kernel smoothing
 the distributions of the two classes
 \item oc_youden_normal: Maximize the Youden-Index parametrically
 assuming normally distributed data in both classes
 \item oc_OptimalCutpoints: A wrapper for optimal.cutpoints from the OptimalCutpoints package.
 Supply an additional "oc_metric" argument with the method choice corresponding
 to a method from the OptimalCutpoints package
}

User defined functions can be supplied to method, too. As a reference,
the code of all included method functions can be accessed by simply typing
their name. To define a new method function, create a function that may take
as input(s):
\itemize{
 \item data: A data frame or tbl_df
 \item x: (character) The name of the predictor or independent variable
 \item class: (character) The name of the class or dependent variable
 \item metric_func: A function for calculating a metric, e.g. accuracy. Note
 that the method function does not necessarily have to accept this argument
 \item pos_class: The positive class
 \item neg_class: The negative class
 \item direction: ">=" if the positive class has higher x values, "<=" otherwise
 \item ... Further arguments
}

The ... argument can be used to avoid an error if not all of the above
arguments are needed or in oder to pass additional arguments to method.
The function should return a data frame or tbl_df with
one row, the column "optimal_cutpoint", and an optional column with an arbitraty name
with the metric value at the optimal cutpoint.

Built-in metric functions include:
\itemize{
 \item accuracy: Fraction correctly classified
 \item youden: Youden- or J-Index = sensitivity + specificity - 1
 \item sum_sens_spec: sensitivity + specificity
 \item sum_ppvnpv: The sum of positive predictive value (PPV) and negative
 predictive value (NPV)
 \item prod_sens_spec: sensitivity * specificity
 \item prod_ppvnpv: The product of positive predictive value (PPV) and
 negative predictive value (NPV)
 \item cohens_kappa: Cohen's Kappa
 \item abs_d_sens_spec: The absolute difference between
 sensitivity and specificity
 \item abs_d_ppvnpv: The absolute difference between positive predictive
 value (PPV) and negative predictive value (NPV)
 \item p_chisquared: The p-value of a chi-squared test on the confusion
 matrix of predictions and observations
 \item odds_ratio: The odds ratio calculated as (TP / FP) / (FN / TN)
 \item risk_ratio: The risk ratio (relative risk) calculated as
 (TP / (TP + FN)) / (FP / (FP + TN))
 \item misclassification_cost: The sum of the misclassification cost of
 false positives and false negatives. Additional arguments: cost_fp, cost_fn
 \item total_utility: The total utility of true / false positives / negatives
 calculated as utility_tp * TP + utility_tn * TN - cost_fp * FP - cost_fn * FN.
 Additional arguments: utility_tp, utility_tn, cost_fp, cost_fn
 \item F1_score: The F1-score (2 * TP) / (2 * TP + FP + FN)
}

User defined metric functions can be used as well which can accept the following
inputs as vectors:
\itemize{
 \item tp: Vector of true positives
 \item fp: Vector of false positives
 \item tn: Vector of true negatives
 \item fn: Vector of false negatives
 \item ... If the metric function is used in conjunction with any of the
 maximize / minimize methods, further arguments can be passed
}

The function should return a numeric vector or a matrix or a data.frame
with one column. If the column is named,
the name will be included in the output and plots. Avoid using names that
are identical to the column names that are by default returned by cutpointr.

If boot_runs is positive, that number of bootstrap samples will be drawn
and the optimal cutpoint using method will be determined. Additionally,
as a form of cross validation, the function in metric will be used to
score the out-of-bag predictions using the cutpoints determined by
method. Accuracy,
Sensitivity, Specificity, Kappa, true positives/negatives and false
positives/negatives are always included in the bootstrap results.

If multiple optimal cutpoints are found, the first one is returned and a
warning including all optimal cutpoints is issued. The first one refers to
the minimum of the optimal cutpoints if direction = ">=" or to the maximum
of the optimal cutpoints if direction = "<=".

If use_midpoints = TRUE the mean of the optimal cutpoint and the next
highest or lowest possible cutpoint is returned, depending on direction.
If use_midpoints is set to TRUE and multiple optimal cutpoints are found,
the midpoint of the minimum / maximum of the optimal cutpoints
and the next highest / lowest observation is returned, as described before. Thus, finding
multiple optimal cutpoints has no effect on determining the midpoint.
}
\examples{
library(cutpointr)

## Optimal cutpoint for dsi
data(suicide)
opt_cut <- cutpointr(suicide, dsi, suicide)
opt_cut
summary(opt_cut)
plot(opt_cut)
predict(opt_cut, newdata = data.frame(dsi = 0:5))

## Supplying raw data
## Same result
cutpointr(x = suicide$dsi, class = suicide$suicide)

## direction, class labels, method and metric can be defined manually
opt_cut <- cutpointr(suicide, dsi, suicide, direction = ">=", pos_class = "yes",
                     method = maximize_metric, metric = youden)
opt_cut

## Optimal cutpoint for dsi, as before, but for the separate subgroups
opt_cut <- cutpointr(suicide, dsi, suicide, gender)
opt_cut
summary(opt_cut)
plot(opt_cut)

## Bootstrapping to assess cutpoint variability and out-of-sample performance
set.seed(12)
opt_cut <- cutpointr(suicide, dsi, suicide, boot_runs = 30)
opt_cut
summary(opt_cut)
plot(opt_cut)

## Bootstrapping also works on individual subgroups
set.seed(12)
opt_cut <- cutpointr(suicide, dsi, suicide, gender, boot_runs = 30)
opt_cut
summary(opt_cut)
plot(opt_cut)

## Transforming variables (unrealistic, just to show the functionality)
set.seed(12)
opt_cut <- cutpointr(suicide, log(dsi + 1), suicide == "yes",
    subgroup = dsi \%\% 2 == 0, boot_runs = 30)
opt_cut
summary(opt_cut)
plot(opt_cut)
predict(opt_cut, newdata = data.frame(dsi = 1:3))

## Different cutpoint function / metric
set.seed(12)
opt_cut <- cutpointr(suicide, dsi, suicide, gender, pos_class = "yes",
  boot_runs = 30, method = minimize_metric, metric = abs_d_sens_spec)
opt_cut
plot(opt_cut)

## Handling of NA values
suicide_na <- suicide
suicide_na$dsi[10] <- NA
suicide_na$suicide[20] <- NA
suicide_na$gender[30] <- NA
opt_cut_na <- cutpointr(suicide_na, dsi, suicide, gender, na.rm = TRUE)
opt_cut_na
plot(opt_cut_na)

## Parallelized bootstrapping (warning expected)
if (require(doSNOW) & require(doRNG)) {
  cl <- makeCluster(2) # 2 cores
  registerDoSNOW(cl)
  registerDoRNG(12) # Reproducible parallel loops using doRNG
  opt_cut <- cutpointr(suicide, dsi, suicide, gender, pos_class = "yes",
                 direction = ">=", boot_runs = 100, allowParallel = TRUE)
  stopCluster(cl)
  opt_cut
  plot(opt_cut)
}


## Wrapper for optimal.cutpoints
if (require(OptimalCutpoints)) {
  opt_cut <- cutpointr(suicide, dsi, suicide, gender, boot_runs = 30,
                       method = oc_OptimalCutpoints, oc_metric = "Youden")
  opt_cut
  plot(opt_cut)
}

## Cutpoint function assuming normally distributed data
opt_cut <- cutpointr(suicide, dsi, suicide, gender, boot_runs = 30,
                     method = oc_youden_normal)
opt_cut
plot(opt_cut)



}
