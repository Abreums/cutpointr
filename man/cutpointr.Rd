% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cutpointr.R
\name{cutpointr}
\alias{cutpointr}
\title{Determine and evaluate optimal cutpoints}
\usage{
cutpointr(data = NULL, x, class, subgroup = NULL,
  method = maximize_metric, metric = sum_sens_spec, pos_class = NULL,
  neg_class = NULL, direction = NULL, boot_runs = 0,
  use_midpoints = FALSE, na.rm = FALSE, allowParallel = FALSE,
  silent = FALSE, ...)
}
\arguments{
\item{data}{A data frame or tibble in which the columns that may be given in x,
class and possibly subgroup can be found.}

\item{x}{The variable name without quotation marks to be used for
classification, e.g. predictions or test values, or the raw data if data = NULL.}

\item{class}{The variable name without quotation marks indicating class membership
or the raw data if data = NULL.}

\item{subgroup}{The variable name without quotation marks
of an additional covariate that identifies subgroups or the raw data if
data = NULL. Separate optimal cutpoints will be determined by group.
Numeric, character and factor are allowed.}

\item{method}{(function) A function for determining cutpoints. Can
be user supplied or use some of the built in methods. See details.}

\item{metric}{(function) The function for computing a metric when using
maximize_metric or minimize_metric as method and and for the
out-of-bag values during bootstrapping. A way of internally validating the performance.
User defined functions can be supplied, see details.}

\item{pos_class}{(optional) The value of class that indicates the positive class.}

\item{neg_class}{(optional) The value of class that indicates the negative class.}

\item{direction}{(character, optional) Use ">=" or "<=" to indicate whether x
is supposed to be larger or smaller for the positive class.}

\item{boot_runs}{(numerical) If positive, this number of bootstrap samples
will be used to assess the variability and the out-of-sample performance.}

\item{use_midpoints}{(logical) If TRUE (default FALSE) the returned optimal
cutpoint will be the mean of the optimal cutpoint and the next highest
observation (for direction = ">") or the next lowest observation
(for direction = "<").}

\item{na.rm}{(logical) Set to TRUE (default FALSE) to keep only complete
cases of x, class and subgroup (if specified). Missing values with
na.rm = FALSE will raise an error.}

\item{allowParallel}{(logical) If TRUE, the bootstrapping will be parallelized
using foreach. A local cluster, for example, should have been started manually
beforehand.}

\item{silent}{(logical) If TRUE suppresses all messages.}

\item{...}{Further optional arguments that will be passed to method.
minimize_metric and maximize_metric pass ... to metric.}
}
\value{
A cutpointr object which is also a data.frame and tbl_df.
}
\description{
Using predictions (or e.g. biological marker values) and binary class labels, this function
will determine "optimal" cutpoints using various selectable methods. The
methods for cutpoint determination can be evaluated using bootstrapping. An
estimate of the cutpoint variability and the out-of-sample performance will then
be returned.
}
\details{
If direction and/or pos_class and neg_class are not given, the function will
assume that higher values indicate the positive class and use the class
with a higher median as the positive class.

Different methods can be selected for determining the optimal cutpoint via
the method argument. The package includes the following cutpoint functions:
\itemize{
 \item maximize_metric: Maximize the metric function
 \item minimize_metric: Minimize the metric function
 \item maximize_loess_metric: Maximize the metric function after LOESS
 smoothing
 \item minimize_loess_metric: Minimize the metric function after LOESS
 smoothing
 \item maximize_boot_metric: Maximize the metric function as a mean of
 the optimal cutpoints in bootstrapped samples
 \item minimize_boot_metric: Minimize the metric function as a mean of
 the optimal cutpoints in bootstrapped samples
 \item oc_manual: Specify the cutpoint manually
 \item oc_youden_kernel: Maximize the Youden-Index after kernel smoothing
 the distributions of the two classes
 \item oc_youden_normal: Maximize the Youden-Index parametrically
 assuming normally distributed data in both classes
}

User-defined functions can be supplied to method, too. As a reference,
the code of all included method functions can be accessed by simply typing
their name. To define a new method function, create a function that may take
as input(s):
\itemize{
 \item data: A data frame or tbl_df
 \item x: (character) The name of the predictor or independent variable
 \item class: (character) The name of the class or dependent variable
 \item metric_func: A function for calculating a metric, e.g. accuracy. Note
 that the method function does not necessarily have to accept this argument
 \item pos_class: The positive class
 \item neg_class: The negative class
 \item direction: ">=" if the positive class has higher x values, "<=" otherwise
 \item ... Further arguments
}

The ... argument can be used to avoid an error if not all of the above
arguments are needed or in oder to pass additional arguments to method.
The function should return a data.frame or tbl_df with
one row, the column "optimal_cutpoint", and an optional column with an arbitraty name
with the metric value at the optimal cutpoint.

Built-in metric functions include:
\itemize{
 \item accuracy: Fraction correctly classified
 \item youden: Youden- or J-Index = sensitivity + specificity - 1
 \item sum_sens_spec: sensitivity + specificity
 \item sum_ppv_npv: The sum of positive predictive value (PPV) and negative
 predictive value (NPV)
 \item prod_sens_spec: sensitivity * specificity
 \item prod_ppv_npv: The product of positive predictive value (PPV) and
 negative predictive value (NPV)
 \item cohens_kappa: Cohen's Kappa
 \item abs_d_sens_spec: The absolute difference between
 sensitivity and specificity
 \item abs_d_ppv_npv: The absolute difference between positive predictive
 value (PPV) and negative predictive value (NPV)
 \item p_chisquared: The p-value of a chi-squared test on the confusion
 matrix of predictions and observations
 \item odds_ratio: The odds ratio calculated as (TP / FP) / (FN / TN)
 \item risk_ratio: The risk ratio (relative risk) calculated as
 (TP / (TP + FN)) / (FP / (FP + TN))
 \item misclassification_cost: The sum of the misclassification cost of
 false positives and false negatives. Additional arguments: cost_fp, cost_fn
 \item total_utility: The total utility of true / false positives / negatives
 calculated as utility_tp * TP + utility_tn * TN - cost_fp * FP - cost_fn * FN.
 Additional arguments: utility_tp, utility_tn, cost_fp, cost_fn
 \item F1_score: The F1-score (2 * TP) / (2 * TP + FP + FN)
}

User defined metric functions can be used as well which can accept the following
inputs as vectors:
\itemize{
 \item tp: Vector of true positives
 \item fp: Vector of false positives
 \item tn: Vector of true negatives
 \item fn: Vector of false negatives
 \item ... If the metric function is used in conjunction with any of the
 maximize / minimize methods, further arguments can be passed
}

The function should return a numeric vector or a matrix or a data.frame
with one column. If the column is named,
the name will be included in the output and plots. Avoid using names that
are identical to the column names that are by default returned by cutpointr.

If boot_runs is positive, that number of bootstrap samples will be drawn
and the optimal cutpoint using method will be determined. Additionally,
as a way of validation, the function in metric will be used to
score the out-of-bag predictions using the cutpoints determined by
method. Various default metrics are always included in the bootstrap results.

If multiple optimal cutpoints are found, the first one is returned and a
warning including all optimal cutpoints is issued. The first one refers to
the minimum of the optimal cutpoints if direction = ">=" or to the maximum
of the optimal cutpoints if direction = "<=".

If use_midpoints = TRUE the mean of the optimal cutpoint and the next
highest or lowest possible cutpoint is returned, depending on direction.
If use_midpoints is set to TRUE and multiple optimal cutpoints are found,
the midpoint of the minimum / maximum of the optimal cutpoints
and the next highest / lowest observation is returned, as described before.
}
\examples{
library(cutpointr)

## Optimal cutpoint for dsi
data(suicide)
opt_cut <- cutpointr(suicide, dsi, suicide)
opt_cut
summary(opt_cut)
plot(opt_cut)

\dontrun{
## Predict class for new observations
predict(opt_cut, newdata = data.frame(dsi = 0:5))

## Supplying raw data, same result
cutpointr(x = suicide$dsi, class = suicide$suicide)

## direction, class labels, method and metric can be defined manually
## Again, same result
cutpointr(suicide, dsi, suicide, direction = ">=", pos_class = "yes",
          method = maximize_metric, metric = youden)

## Optimal cutpoint for dsi, as before, but for the separate subgroups
opt_cut <- cutpointr(suicide, dsi, suicide, gender)
opt_cut

## Bootstrapping also works on individual subgroups
## low boot_runs for illustrative purposes
set.seed(30)
opt_cut <- cutpointr(suicide, dsi, suicide, gender, boot_runs = 5)
opt_cut
summary(opt_cut)
plot(opt_cut)

## Transforming variables (unrealistic, just to show the functionality)
opt_cut <- cutpointr(suicide, x = log(dsi + 1), class = suicide == "yes",
    subgroup = dsi \%\% 2 == 0)
opt_cut
predict(opt_cut, newdata = data.frame(dsi = 1:3))

## Parallelized bootstrapping
  cl <- makeCluster(2) # 2 cores
  registerDoParallel(cl)
  registerDoRNG(12) # Reproducible parallel loops using doRNG
  opt_cut <- cutpointr(suicide, dsi, suicide, gender,
                       boot_runs = 10, allowParallel = TRUE)
  stopCluster(cl)
  opt_cut
  plot(opt_cut)

## Robust cutpoint method using kernel smoothing for optimizing Youden-Index
opt_cut <- cutpointr(suicide, dsi, suicide, gender,
                     method = oc_youden_kernel)
opt_cut
}



}
